{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ak2742/code-completion-using-google-codegemma/blob/main/CodeGemma_Gradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CodeGemma 2B & 7B\n",
        "\n",
        "CodeGemma is a family of code-specialist LLM models by Google, based on the pre-trained 2B and 7B Gemma checkpoints.\n",
        "\n",
        "They were trained on top of the base Gemma 2B and 7B models using a mixture of 500 billion tokens of primarily English language data, mathematics, and code.\n",
        "\n",
        "They leverage the natural language capabilities of their ancestors, improve on logical and mathematical reasoning, and are suitable for code completion and generation.\n",
        "\n",
        "*Note: make sure, you're on a GPU run-time with Colab.*"
      ],
      "metadata": {
        "id": "-twODo7GxuM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup the inference environment"
      ],
      "metadata": {
        "id": "SYFHJ4KdzxxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade transformers\n",
        "!pip install -q --upgrade gradio"
      ],
      "metadata": {
        "id": "8FW4dEMMzWX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the tokenizer and the model\n",
        "\n",
        "CodeGemma 2B was trained exclusively on the Code Infilling task and is meant for fast code completion and generation, especially in settings where latency and/or privacy are crucial.\n",
        "\n",
        "CodeGemma 7B training mix includes code infilling data (80%) and natural language. It can be used for code completion, as well as code and language understanding and generation.\n",
        "\n",
        "CodeGemma 7B Instruct, in fact, was fine-tuned for instruction following on top of CodeGemma 7B. It’s meant for conversational use, especially around code, programming, or mathematical reasoning topics. It’s not as powerful as the other versions for code completion."
      ],
      "metadata": {
        "id": "k95vA7tEz3XO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNSmWr9Fxhzc"
      },
      "outputs": [],
      "source": [
        "from transformers import GemmaTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "#device Check\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "\n",
        "model_id = \"google/codegemma-2b\"\n",
        "\n",
        "tokenizer = GemmaTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "\ttorch_dtype=torch.float16\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Guide for Code Infilling i.e. Fill in the Middle.\n",
        "\n",
        "Code completion can be used for infilling inside code editors. CodeGemma was trained for this task using the fill-in-the-middle (FIM) objective, where you provide a prefix and a suffix as context for the completion. The following tokens are used to separate the different parts of the input:\n",
        "\n",
        " - `<|fim_prefix|>` precedes the context before the completion we want to run.\n",
        "- `<|fim_suffix|>` precedes the suffix. You must put this token exactly where the cursor would be positioned in an editor, as this is the location that will be completed by the model.\n",
        "- `<|fim_middle|>` is the prompt that invites the model to run the generation.\n",
        "\n",
        " In addition to these, there's also `<|file_separator|>`, which is used to provide multi-file contexts.\n",
        "\n",
        " Please, make sure to not provide any extra spaces or newlines around the tokens, other than those that would naturally occur in the code fragment you want to complete. Here's an example:\n"
      ],
      "metadata": {
        "id": "a6ka6Ccl0uWg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate code fn\n",
        "\n",
        "We tokenize the inputs and pass them through model.generate. You can pass a wide variety of arguments and strategies, read more about them [here](https://huggingface.co/docs/transformers/generation_strategies)."
      ],
      "metadata": {
        "id": "Lwb4BTB61mmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<|fim_prefix|>\", \"<|fim_suffix|>\", \"<|fim_middle|>\", \"<|file_separator|>\"]})\n",
        "\n",
        "def generate(prompt, return_full_text=False, max_new_tokens=1200):\n",
        "  inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "  prompt_len = inputs[\"input_ids\"].shape[-1]\n",
        "  print(f\"Input Tokens: {prompt_len}\")\n",
        "  if return_full_text:\n",
        "    response = tokenizer.decode(model.generate(**inputs, max_new_tokens=max_new_tokens)[0], skip_special_tokens=True)\n",
        "  else:\n",
        "    response = tokenizer.decode(model.generate(**inputs, max_new_tokens=max_new_tokens)[0][prompt_len:], skip_special_tokens=True)\n",
        "  return response"
      ],
      "metadata": {
        "id": "z1lpeePINXvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Add Gradio UI\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "def gradio_fn(code, return_full_text=True, max_new_tokens=1200):\n",
        "    msg = f\"<|fim_prefix|>{code}<|fim_suffix|><|fim_middle|>\"\n",
        "    response = generate(msg, return_full_text, max_new_tokens)\n",
        "    return response\n",
        "\n",
        "gr.Interface(\n",
        "    fn=gradio_fn,\n",
        "    inputs=[gr.Textbox(lines=4, label=\"Input Code\"),\n",
        "         gr.Checkbox(label=\"Return Full Text\", value=True),\n",
        "         gr.Slider(minimum=0, maximum=2000, step=100, label=\"Max New Tokens\", value=1200)],\n",
        "    outputs=gr.Textbox(lines=4, label=\"Output Code\"),\n",
        "    theme=\"soft\"\n",
        "    ).launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "zw9h2EfLRSoH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}